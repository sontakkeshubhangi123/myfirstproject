<!DOCTYPE html>
<html>
  <head> </head>

  <body>
    <h1><mark>Artificial intelligence</mark></h1>
    <hr />
    <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
      ><img
        src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGBxESEhIPEBAWEBAVFRAQFxUVFRUVFRkZFRUWFxcWFxcYHSghGRolGxcWITEhJSkrLi4vGB8zODMtNygtLisBCgoKDg0OGhAQGi0lIB0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLSstLS0tLS0tLS0tKy0tLf/AABEIAJoBSAMBIgACEQEDEQH/xAAcAAACAgMBAQAAAAAAAAAAAAAAAQIFAwQGBwj/xABMEAACAQMDAQcABgcDBwoHAAABAgMABBEFEiExBhMiQVFhcQcUMoGRoSNCUnKxwfAzYuEVFoKSorLRJCVzhJO0wsPS8TQ1Q1NjZIP/xAAZAQADAQEBAAAAAAAAAAAAAAAAAQIDBAX/xAAnEQACAgICAQMDBQAAAAAAAAAAAQIREiEDMUETIoEEUXEyUpGhsf/aAAwDAQACEQMRAD8A8ZopUVuZkqKKKBhRRRTEx0UUUCCiiigAp0qKAHTpU6BBTAopimgHRRToAKYFIVM0xCoopimIYFMCimBTAnFWzHNtPzWCDqK2ZMZ6eVaR6MZdkIOXJ+abryayWceWOPerXSdCubhmEFu8w6bgMJ8FzhQfk1VaIctlMoqy0uPmu20/6MbkpiaWKHPkMyt9+MD8zXQad9H1rCo3OZpOpZwQv+iiMMfeTVRlFMxm200U3ZvRTcFYxjJBPPt1rN2i7KNHkAZx6V0cWn/VvHEyBR7Orcnpu3GtW+1VM5uAdh47xWLbf3vPHuOR+ddXqSbtfpPJcZKXnK/ijy28tihIxVLdL1r0ftFpK47yN+84LcHJI9Rjr/XWuB1WAKeDuU8g+vOP5VjyNPo9Xgb8lfbDrWGfzrNE2M1rzNnNcr6OtdmvSp1c6PocM6b5NRtbVmYxpHKZCxYY5fYpEScjxtx19Kybo3WylIpV1X+ZMkYzfXUGnEySQRrMXYyNG21mHdqdsW7jvDxXMTx7WZNyvtZk3IdyHaSMq3mpxkH0pWmFEKVOlQBCinSqSiVFIU6YwooopgOilToJCiiigAooFFADp0hToEMVKkKdNAFOkKYpiHUqVOmIKkBURUxTAKmBSArKiVSRDZKBea6Ts52Sur5/0CYjHDSvlY19s/rN7DPlnFdF9HX0eG6C3d0Clt1ROQ0vvn9WP36nywOT7TFBHCioihEUBVRRgADyCilLkUdIypt2cT2c+jKztsPMPrcvrIMRj92LoR+9urory7SEBWIjH2VUefsqjk/AFVfbnU7+OJfqEal2ba2SDIoI+0oYhevHJPUVzOiaXKkhubpu8nwwDM29/F1JPQccYXjBP3qNvslxT3Z1rX2eQCB78ffj/jg1rvcmtJpzgAnJ4yemfeuG7adsBHm2g8TnKyOp+xkfZBx9o/lWiJUW+i31/tSpbuUIKgkH+8Rjj3A5+8fGdSBd0M03erGE/UZjukds+BVxuOeeen5kcNZSAKrONwcNtZv7hGfPGcY6/wAxV1p6q8gDFo42VwzAbyo2FtwCeJ+QAQPLkDANdOdQ9pi/p1KdyJSXvdMGQYU7TtzyhP7J/ZzVfqqCXBGAzEKR5AswA+Mkj8TVhLFG8eVcOVyPcjzBFV/cOylTIcAMmD0B6q4Prkgnz54z5Tk5Lo29NccuznZEIOK13WujlsDIDIv73+tgsB+6xIqmuI8A1hKJrHkV0V1d/oPbmBLaCCYTwtAO7zbwWUizIDld5nXcj/q5G4HGcZrgwtRIrCUbOlOj05/pSS4GboT2zq8m36tFaTBombcsbfWFyrKOAy8HqRXBdqNY+uXU113YhVyu2NcYVVUKo4ABOBkn1JqrFOpUaG2Rop0qoRGkaKDUlAKlUBUqAHRRSoGMU6VFMTHRRRQIBTpUUAMU6Qp0CJCnSFOmACpCoipCmhEhRSFMUxDAqYpCpAVSEzIi13n0Y9jvrsxlmXNpERuH/wBx+oj+MYLexA8+OJt4ySABknAAHUk9APevpzsto62VpDbDBZV8R/aduXb4yT9wAonLFaMu2WhIQAAAYwAOgA/kKpL/AFPk7Tk/tfyHtUtZu8DYDyeTXN3FxU8fH5ZnKblrwZ7i5zyTk1ValqccKGSVwij16n2A6k+1c32p7Zx25MUY724xnbnwrxnLn45wOfiuCmiurzdcTeP9nPHAOCsajgD+OPOtHL7FR4r29F9rnbmaXKWq91HnaXb7Rz/uA/1iqOxh3o6bdrjxOXfw7ufGDj9nORznAPPGK3aQAPL09Sfb16Cty1kkRNxxnJIUgjw8Z49PsnipTN8Ulo3I2Kd2Fydu8p5bd+AeB1JCjnP/AAq20++w25mVyMnHBB4PhbyKnkeuDXOpcmRiw4UkjAJGMjnqc9M1mhbG8hcL4V+8Z/8ASTW0XoxnE7Ds/DEz8rtTLYAwWHPAJ4OMZ8/Tk+djq1gquQMHem4em6Mj88Oefaub0a+wd2RtGGGRwc5x/XxW/qGtd48BzghnU+njXaOfkj8K6XO0meXPg5PX10yqe6EayA5xv8vSQFv476o7zODn0/PofzBqygcNJ04Ijb4KyqMY9i5/Cqq7Y7efUjr6gHn799cs5bPS44UrK8GommaVc50kQKlTAoNAGM0UGikUY6KKKlFiqVIU6BHo3YTR7Oa3gubmFTHDLc2sxxy5uXtY7dm9dnfyEf8AR+1bMvZ+zWJLK4iaORZdEtZHhWPvO+uIbmRyzODwO8TOOvdqOOo84iupQjQpI4jdlZo1ZgrMv2SUBwSPI44rLPqVyWLPPMXZ0mJaSTcXQEJISTkuoJAbqPKpxY7R1Vp2Mh+r3Ek0kiyxC7lXDRASx203dyFI+XGdrDexADcYas93o9sNce0t07uJfrKlXSORQyW0r+FGBG3hcZyQcn0rkf8ALl33bQfW5+5YuWj72TYxfO/cucHdk5z1yfWlLqV0zLcNPOzovdLKXkLKuCuwSZyBhiMZ/WPrRTAstY0WGO0trqB3mDiJZXLxlUlaLeYjHgOjAhsE5DBSQfKqCugubHVJrJbyXv5tPiPdq7y7kQ57vwozZxnC7sYHTPFYtF7I393GZrW0eaIEguNqrkdQCxG7HtmmmJlIKdWmq9nbu2ihnuLdoopxujYlTuGAwyASVODnDYNVdUhDFFWcvZ27W1XUGt2Fo7bVlyuCckDw53AEgjJGD94qsoAkKdWGjaFdXXefVYHn7sKz7ceENnBOSP2T+FVwNMQxU6gKnTRIVIVEVIUwJVNahU0qkTI6f6PrUS6jZo3I70P/ANkDIP8Acr6OmbGK+fPosH/Olp8z/wDd5a+gLqRQviOBU8m2jCTo5bWpfGa5+5lNb+rXgDEFvx4/M8GqeSUE/wDGtkZpHm3bDSHiuHucbopcnP7LEYIPz1H+FYtFuJQmyMF2clY1Az4hyzEnhRj144Y+Rr0aaJWBRlDKwIIIyCD5EVx+t6Otu6Tr/wDDggOFO1158PiwfBkDyyPzEY07R0x5FJUzS0RjFI08ijvNxxuB4/aIyOD0Geo56VkuSJThPFIx8KqOrZ4xj3x+GfUiunuSx2KBkZ4UkgDk4BYk4+/766jsHpoCm5l5Lbsf3UBwQPkqc+wHkTlr7ClrZgsuyjhcsQuRyEPhyeviOc+XQY44zWC+0I4wrZ29MEN5cZAx55rc1fX3nLMpKRhtiKoxuxj2x/QxnJIrBcMMOuecfa59uPUdapLRGVvZpSQFAq5Ln7/U5/jx99ZLm7AiMuMyI0b88gnxOCfnC/ODisupwZjVs8liST1/w52/hVdFAZBgdGkiQ9PtNkfgck07pFqNtFpdOqzIB+pujPyis/5lR+NU+opgyL0KkN9ynaB8/pBXR61ysBOMi4kjwBghXZeGPnyZPjJqj1kZdz5uqEDzJkaN8AeeNtZSLiU1GKKkDxUFGMGg0U8UDIUUzSpDMdFFFQWFSqNOmI6PsLbBrgzN9iFGfPoTwPy3n7qz9r2E9va3yjllMbY555IGfZg4++paBqkVrZySKyPcO4/Rk84BCjIHOMbm++s0muR3dncRzmOGQcxqDtB2gMuNx6lgR99elDD0PTvbTfz43+EebPP1/Ux0ml8edfl/0dXe/RrpVrFbXd9qE0FvLFGSvhaRpHVWwm2M4QDOfCeo5Hn0GmaZpQ7OlZLqZLCR0lklCkyLJ3qAqAI+QJFC52ngffXI/S7rVtcWukpb3EczxRuJFRwxQmOEYYDpyD+FbvY/UbG80OTR7i+jsJlkLbpioBHfCYMu5lDea4ByMZ9M+S7o9Q0m0cf5vS3Ed9cG2FyQICU7kr9aWMMV2bs4w+MgbucV2vaeWzttAhFvfzwQbG+rSRBkeeQxyukcmEyFdsluF6ckVx8+qWkfZubTxdxSXC3DBUVxudVvAwdVznaVG74rW7Z6zbSdn9MtY543uI3hLxKwLqBDMDuXqOSB94o7A2u2/Zid7bRIY7qe5e4VI445mTu490UZ42qDgZ6tuIUVi7Sdi9FsEe3udSnOoLF3u2NAULEEqu3YQM+hcHGDkZq17Tds7WKPQJ4ZkuGtdhmjjdWdR3EaOCM8NjcBnzFYe3OlaRfyzarHrUMJkiDGFhukMiR7U8O7eoO1AV2E8HHXgTBlTq2lSL2btbn63OyNKMWxZO4BMko4AXd5ZALEAnpVl2Z7AaLeBYYtRuZ7hkDl0hKwqSu7axMZUEDqpfPl1rW1DW7Q9nLK0MyPOkyNJArjvdomlZvDnI8J6+4r0Edq9PWazmh1mC309Y+7FkiIMsQwUyH7USqCPCQoBX3obYHNfQ1pj2tzrVrIQXhWKIkcA7TPhgPIEYP314tH0HwK9r7IdqLFNX1kyXUccVz3fdTM6iJtgYHxk4z4+OecGvHL23WKSSJJBMkbyRLIv2XCMVDrgnhgMjk9auPZMjEKlUBU60ICpCo1JaYiVTWoVJapEs6r6Ob1YtStHc4Uu8eT6yxvGv8AtMtewzasFa4eXgo2xB7AfzPNfPIr3LTrZryys3lBEssKl282wSFc8YywAb76rV7MJo5fWdXZ2xnk+WP5VtabZShdz5weldNZ9i445C742jkKvn0ySfIcdPX8tDX9RHTgKBgAcYAq8r6IdeCpnkAzXPa3ehlaPyIINGo6iT0qlml8z60FpFRLcE+FyWK4CsSSQqHAU59v4cV3nZ65DQ7M8mOPP+lGufzzXIyWiP1yM85HX/hUlZkTuVlCEgorEYGM52n8SPj4qVo0l7hWbdV3tt3FlUdMkjJx1ycD8K2+46AZ9AP66VJdLlkALRBT0LrImw4GMnJz6eWaslubaJSquryKnXLMpboAxGercbRk1d6MnHeis1yTuhEjIWHLHBwR0x1BHQ9CPMdKroiB4lPnkDaFIKjIO0cVZ2FhJLIDIdzSb5nzyAFQnjjpwP4V0J7NbYu8AKk4HTpgA5H3/HFXDjyWzPk+qjwyS8lT2pvhIcJGEAImbHAMh2M5yfIMzL9xrm9S6ttO5u9XYc8gDdn7vsYroNVtV7rvV83KMPMFcFgeeR4geMda55lxF3gPjBlTPt+jH44dh8fFZThR0w5c3Zo3RBdyv2S7kfG44/KsJp0qwNRYoNMU2pgYzSpmipKMRoooqCwp0qdMRb6HoTXKyuJAndgHBUnOQT68dKpwwrs+wX9ld/ur/uvXcfRleCDs7eXBhScwzyzKkgyhZFgZCR7MAfurfmjGPFCS7d/6YcU5S5ZxfSqv4PF3Uj7QK5GRkEceoz5V0vaPsc9nZ2V88yut2quECkFN0YcZbPPB9BXpvafXXu9AtdZlij+twXEU6YU7A0dyY8YznawUZGf4CrH6UO2dzbabaSxpEWvIiku5WIAkgDHZhhjqeua5cmdNHz7GhPCgsQM8Anj148q6X6OezkWo3q2kzukbRyvujK7soAR9oEY59K9tsrEaZY2UNlc2VkXVZJZLs4MzbVLEeIE8t6naNoFVFjHaf5zRS2TxOktnLJJ3LKyd5llY+E4BICk/j50OQsTjuyX0dW13qOo2Mk0yx2jbUZTHvbxsvjyhHQeQFc7oNjpTWd895cyR3qbxboucNhfBxtIYl8ggkYHPHWvVPoz/APnmu/8ASf8AmvVH9GY/5k1z/rn/AHai2OjyCNC3CgscZwASfnApA17t2MW5Gh2raEkRuzMBcmQKCSC3ebixGcfo/fZ05xVn2imh1DWrXS5DHNbW8bXsiYB/TJvURs2eRh1YoR6Z64p5CxPnjacA4OD0ODg/B86ZNfTN5dwSvdWmoX2nNYsHhWASKk0ZU7RuLPww56AFWAx0rkOxOo/VuzUl33Mc7wyySIso3Jv75VViP7pbPl06jrTUxYnjDKRwQQeuCCDj15qSKTnAJxycAnHzjpXs3b6f/KOh2N9MqJcvNCu5QcL3jPE2MnO04DYz5D0rc7d9pm0EWdjptvEsZRpGaRWbdtIXkqwyx5JYknkU8xYnhi84A5J6Ack/HrUsdQeCOCPP769y+jmAJpk2rLLbQX11NMzXFwAsSfpymwAEbQSCQoIyWHXAFUX0xNbS21ncfWLWfUA/czNbOpDIUdtxUEnAZRjPTeR501PdA46s8spg1EGpCtEZsueyukfW7qK3OQjHdIRnwxoN0hyOnhBAPqRX0JodwHd8AKoCqqjoqjgKB6AYryLsNqEFvbM6LuuJXKTv+tHErAqij0ONxPmeP1eOt0vtZC08aWcbbWdI2LE7juOMkHz6njjiqxbRzcj3R2naC42RkDqeK8n1y4JJr0TtRLwoPXBNeb6omSauCpELs5u8mCgsxwB/WKo7a7EjnvW2rg+uMddvsfQ1ua1CWlEbZCBDIMebZx+XH9Gq5oxudOhHn6YrOTdnXBKi1sD4Tzkbjg+xwf45P31muItyEef9fyzWPTZEbEanBA4HTIFWDw/ozyBvVwpOcElTjp5dK0XRm9Mq7nbthb/9eJ+v7KCPp8qahpsO9htPIGQPLJ8z6YXz9xWK8mEjEJzGoESE9doZjnjzJZsegz6V0fZ+04zjA6D8Sf4k1MU38FyaXydB2csFG5yzOUjkAyeOmRn15xx0q61LtIhiaArypJH5cfwqEcKwwt5MzRxZ95GAI/BTXnmr3hDxuf1oraUj96Ncj8c1vGS8+Dg5vp/UdplvrcqmHcnGTuI+Wdc/J2H8BXPXUOUd84xEhI+UQ/iTmt0yHDIT4fEPgMNyj/WWT8a0bwf8nJ9Y7X8lhrLklZ1cPHjooaVZI1zT7usaN7MQNMigCm9AGI0qDRUloxinSp1BQUCiihDLnQdd+rLKndd53gAzu24wGH7Jz9r8qttF7bm30u50j6tv79pG77vdu3eqD+z2HONn7Q61yFMVcpOUVF9LoiMFGTku32dfJ22J0ddF+rcBt/f97/8AnM2O72e+Pte/tVun0lxSafHp99pqXbRR91HKXAxhCivgoSrhccg8kZ4rzqioxRVs9C0T6TEFpHYanp8eoxQgCJmIVgFGFBDKQSBkbgQcYBB5NVdn22WDU11S2sYrZANhto2whUpsPiC4Vj4TkKBlQcHJzyAp0KKCz0ex+lBINTn1GGxCRXEaxyw94AzODnvQwXG7PljnJ5rVs/pEjhg1O1gsAkV60xUCYKIRLF3eAojwwHXGR1x71wVAoxQWz3nQbQJomnJNpzasjb5gsGFMW8sw3eIFm8bKSMeefU0/b6zttJ/yfqWnw/UL9ny1sz7wYzG28Ou4jAO1TtI+0fMDHl2n63dwKUt7ueBTklY5pI1yepwpAz71q3Ny8jGSWRpXPVnZnY/LMSTSxdjyPQ9b+kWxuUld9CgN3KjRtKzg4yu3eCIw27GMHII9eKp7LtsY9Jl0b6tuEjFu/wC9xjMiyf2ezn7OPtedchTFVihWzrtQ7bGXSodH+rbRGyt33e5ztdm/s9nH2sfa8q6G2+lZJIIotS0yLUJYfsSOVGSBjLKyNtJwMkcH0HSvMqKeKFkzvezf0jG3FzBLZRT2FxJJL9WyQsZc5KoWBBXp4SByMjHSqftdr1rdGIWmmxWCR7/sEFn3Y+3hQOMcdTz18q5sVLNNRVibZIVIVjqQq0SZopCp3KSrDzBIP4ivZPo6mgW0iuJtonYyndgZwrsgPHQ+E/0a8YFdn9Ht5CGmW5csioDFBnCu7HBPrxwcA+ecGriY8sdHfavqqysSpyK5e8bNXEAwrsIUjEn7I8gem48nmqK9zWtHOmc/rtuWTKfbXke/qKqLfTWYAfrdT55J8yent/XPRuBwWOMkgDIBY+gz/H/AHTjV1XIzg7juAxgA8gHoAM4yfz61Ljs3jOka9ppqo2ZCVbyUcnlT1xx5g+lQ1u4AyPPaEHsP1vxOfmia/Iwq8Dr0JLc9R/jWhLEzFc8E+IDqQB+sffp+FJ6VIpK3cjNplqSVB4A/wyfyArtdPXbt9BiqTTYMD3q7hcDqaKrQrvbLTWxJLABBh5UuIpgucFlRSOM9SCwOK4XtGu4JIOQMx5HHhI3x4/0SfwrtElf6vO0LbZ1AKHAPXGQM/rEZx71x1oe+haL9YAKPxLIfjdkfDUCXZg0+XcE56juvL7S+JPvONv8A/SoXRzan9yP/AGZFj/8AAa09Nk5KZ27sYPmGHTHof54qyvY827kcblBx5ArLLLIB7A7gPYD1rORrE56JsUzJWNRnpSIpWx1snuputYk61ORqAowvRSNFSWY6BRRWZQ6KKKBhRQKKYh06iKdMQUUUUAOiiigAp0qKYDp0qdIRIGnUKkKaYDqQNRoFMROmKhmpZpiJA1MViqVUmI9rtr9G06yKtuPcR7j1JcLtfJ9dwbNUiWLztnBVPzPxXB6TrE0XgVsxk52MMjJ6keh+Ktp+0NxJ+jMpWPH2Vwg+Dt5I9jW8Ho5JwaZsaqY0d95USLuVUJyOCQF2rk48ySBnJ+6uhTvOXYM5ywUYC8ew54HOTyADngVTDAdscDJqw09SQcNtbIKn0YdD8evtmpuzR+02Y4kOJMHI3DaQByM+XXyOf/fGnGjST8npknHoOMfjVs6s7ZPhJ2MQeQhKhj58nc7AD2PTnGxoVsGEjqPCZFjX3CLyc+fJB+8/dT2kJOmzLbfa2dXwCeOOehz5dDx7is162OOtS0aPdvm6h3bb+6vgH5LmtFXMkso28kKg5Y7QCSzegPCAezH5JTYrSLLTbphjnAyG9yR0yfSqvVLL6vOLhB/yaU7Wx/8ATZj0PoM8g/d5c3EViRW9HH4cEBgRtZSMgj0IPWoZpo891i2KTMRwGxIMepPP+0Cfgit23YSKykf2isvwxCrJ9+wbh7Rn1q97QaKCgkiBITquSWUEcnnqOF9+vXrXPQdCF+0CGXPTOeh9icA+xNS46CM/cUln5/dUpB1qd0dsshH2WJdc8cN4gPkZwfcGsRbOTUrotrZAkVjkOaRzUSalstIRopUVJRGlTpGsykMUUhRTGOlRRTQgp5pUUwAGpUqKAHTpU6BBRRRQA6KVOgQ6dRp0ASzTqNOnYDp0qKYiWaeajRmnYGeE8ituGXBOa0YzWVm5rSLoxnGwzlj8ms0UhGQDULNSWNbtlpU88higheZ+OEUnHuT0A9zTXVilV0bW53Hg6ldpzxyM9PlSTn+6fSuwl05oLdIIxmVYvgd5INxOfYkfhWLS+xd7GjG6h2ZCqp7yMsP7oAPXj34zXQaoAXkcnCgsc+QANXExlLdFAyCKOK3U4ZtsK/GPEx9gMnPxVhcWtugiaOYGUugk45OCDjJ/OqCH9NOjvJ3UbuIVbBOxMjcxPQE5zj2FXH+b8k08q2wZ7XKBJn8O8bRuI4GRnOcD09zXTBxWmcnPCUmpJ9F4bL2rE9p5AVftB3ceZGACqNzHgcDk/jWhZySy72e3MKAgIWI3vjOSV/V8q5LOmLb2UkqMuSAcjyHWuS1CyjfNxD4ByJF6YzwWA/VI8x7cV289rcd2u7u++wS3B2g4JAGOTzgZ+a5nWdOkAMgG2Qgq2zJRgRzuHX4OD/OizVHG6rBkB+h5Yj5bbIPhZc/9oaq92K6WRMxMWIyrsSoPOxkUORjryWPyc1zM0ZUsp6gkfh5/FYvR0dkDSK0VMCjsfRhNFD9aKkojSp0VmUKig0CmMdKnSNMQUUUUAAozSFFMCdFFAoBjooooEFOlRQA6KKKBDp5qNOgCQNFRqQpoB0UqBTAyRmsklYo+tZT1qkQ+y/7B6K15drAMhDzIwGdqjqfk9B7mva9Tv7fThFY2irHNKHZVwWwEUl5pMcucKcDqSMVy/wBBEa9xdttG7vlXOBnAjUgZ9AST99BYnXNQJJJWG3VT5qCYeB6Dk8e5q4+50cfPLHKX2RsaJp97dXP1uV2WJDtXeeWI5IC9AucAkY5HtWz2rm0yGM291cEEkEhCTJ4TkDCgkDPrXV68xS1nZDtZYX2leCOPIjpXgOloHvrNZAHDTDcGG4HLDqD1q08iOJZVZ3MOqWyNizsl6NL3s5JCg+IsFOdowc+XWrX/ACXqk9xF3k2yAqkrjG0qD0QqAMueu0lsfreQJ2J8f1kv4z9Zdctycd8BjJ8sADHtXojdTVcksaSMOPkynJftZWX8sUEbSysEjQZJxwPgD+Aqr0zV4LuLv4CxTLKNyMuceYyMFfcEitvtbCrxxI6h0aaIFWAZT4vMHg1hvRgYHA6YHAxjpisUdJTavfLH4ndY0Ax6kn5OMD2wfmuTvO08GW2v3nIBxjAz6ev51T/STI3fAbjgJxyePiqC1UC3UgAEk5Pmfmqo2jHVlteXdpOSQxhkyDuwBn9714PzzVDrOntGVOAQQwBByCE6H/U25H901g1MYkOOOE/hVpp5zbvnnElvjPlmXacemRx8VMkbR0UITGDQ7VmbpWGSlQdmu5p1F6KzZqj/2Q=="
        alt="Artficial Intelligence"
    /></a>

    <p>
      Artificial intelligence (AI), in its broadest sense, is intelligence
      exhibited by machines, particularly computer systems. It is a
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >field of research</a
      >
      in computer science that develops and studies methods and software that
      enable machines to perceive their environment and uses learning and
      intelligence to take actions that maximize their chances of achieving
      defined goals. Such machines may be called AIs. AI technology is widely
      used <strong>throughout industry, government</strong>, and science. Some
      high-profile applications include advanced web search engines (e.g.,
      Google Search); recommendation systems (used by YouTube, Amazon, and
      Netflix); interacting via human speech (e.g., Google Assistant, Siri, and
      Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools
      (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy
      games (e.g., chess and Go). However, many AI applications are not
      perceived as AI: "A lot of cutting edge AI has filtered into general
      applications, often without being called AI because once something becomes
      useful enough and common enough it's not labeled AI anymore." Alan Turing
      was the first person to conduct substantial research in the field that he
      called machine intelligence. Artificial intelligence was founded as an
      academic discipline in 1956. The field went through multiple cycles of
      optimism, followed by periods of disappointment and loss of funding, known
      as AI winter. Funding and interest vastly increased after 2012 when deep
      learning surpassed all previous AI techniques, and after 2017 with the
      transformer architecture. This led to the AI boom of the early 2020s, with
      companies, universities, and laboratories overwhelmingly based in the
      United States pioneering significant advances in artificial intelligence.
      The growing use of artificial intelligence in the 21st century is
      influencing a societal and economic shift towards increased automation,
      data-driven decision-making, and the integration of AI systems into
      various economic sectors and areas of life, impacting job markets,
      healthcare, government, industry, education, propaganda, and
      disinformation. This raises questions about the
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >long-term effects, ethical implications</a
      >
      , and risks of AI, prompting discussions about regulatory policies to
      ensure the safety and benefits of the technology. The various subfields of
      AI research are centered around particular goals and the use of particular
      tools. The traditional goals of AI research include
      <strong
        >reasoning, knowledge representation, planning, learning, natural
        language processing</strong
      >, perception, and support for robotics. General intelligence—the ability
      to complete any task performable by a human on an at least equal level—is
      among the field's long-term goals. To reach these goals, AI researchers
      have adapted and integrated a wide range of techniques, including search
      and mathematical optimization, formal logic, artificial neural networks,
      and methods based on statistics, operations research, and economics. AI
      also draws upon
      <strong>psychology, linguistics, philosophy, neuroscience</strong>, and
      other fields.
    </p>

    <h2>Goals</h2>
    <hr />
    <p>
      The general problem of simulating (or creating) intelligence has been
      broken into subproblems. These consist of particular traits or
      capabilities that researchers expect an intelligent system to display. The
      traits described below have received the most attention and cover the
      scope of AI research.
    </p>

    <h3>Reasoning and problem-solving</h3>
    <p>
      Early researchers developed algorithms that imitated step-by-step
      reasoning that humans use when they solve puzzles or make logical
      deductions. By the late 1980s and 1990s, methods were developed for
      dealing with uncertain or incomplete information, employing concepts from
      probability and economics. Many of these algorithms are insufficient for
      solving large reasoning problems because they experience a "combinatorial
      explosion": They become exponentially slower as the problems grow. Even
      humans rarely use the step-by-step deduction that early AI research could
      model. They solve most of their problems using fast, intuitive judgments.
      Accurate and efficient reasoning is an unsolved problem.
    </p>

    <h3>Knowledge representation</h3>

    <p>
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">
        Knowledge representation</a
      >
      and knowledge engineering allow AI programs to answer questions
      intelligently and make deductions about real-world facts. Formal knowledge
      representations are used in content-based indexing and retrieval, scene
      interpretation, clinical decision support, knowledge discovery (mining
      "interesting" and actionable inferences from large databases), and other
      areas. A knowledge base is a body of knowledge represented in a form that
      can be used by a program. An ontology is the set of objects, relations,
      concepts, and properties used by a particular domain of knowledge.
      Knowledge bases need to represent things such as objects, properties,
      categories, and relations between objects; situations, events, states, and
      time; causes and effects; knowledge about knowledge (what we know about
      what other people know); default reasoning (things that humans assume are
      true until they are told differently and will remain true even when other
      facts are changing); and many other aspects and domains of knowledge.
      Among the most difficult problems in knowledge representation are the
      breadth of commonsense knowledge (the set of atomic facts that the average
      person knows is enormous); and the sub-symbolic form of most commonsense
      knowledge (much of what people know is not represented as "facts" or
      "statements" that they could express verbally). There is also the
      difficulty of knowledge acquisition, the problem of obtaining knowledge
      for AI applications.
    </p>

    <h3>Planning and decision-making</h3>

    <p>
      An "agent" is anything that perceives and takes actions in the world. A
      rational agent has goals or preferences and takes actions to make them
      happen. In automated planning, the agent has a specific goal. In automated
      decision-making, the agent has preferences—there are some situations it
      would prefer to be in, and some situations it is trying to avoid. The
      decision-making agent assigns a number to each situation (called the
      "utility") that measures how much the agent prefers it. For each possible
      action, it can calculate the "expected utility": the utility of all
      possible outcomes of the action, weighted by the probability that the
      outcome will occur. It can then choose the action with the maximum
      expected utility. In classical planning, the agent knows exactly what the
      effect of any action will be.[38] In most real-world problems, however,
      the agent may not be certain about the situation they are in (it is
      "unknown" or "unobservable") and it may not know for certain what will
      happen after each possible action (it is not "deterministic"). It must
      choose an action by making a probabilistic guess and then reassess the
      situation to see if the action worked. In some problems, the agent's
      preferences may be uncertain, especially if there are other agents or
      humans involved. These can be learned (e.g., with inverse reinforcement
      learning), or the agent can seek information to improve its preferences.
      Information value theory can be used to weigh the value of exploratory or
      experimental actions.The space of possible future actions and situations
      is typically intractably large, so the agents must take actions and
      evaluate situations while being uncertain of what the outcome will be. A
      Markov decision process has a transition model that describes the
      probability that a particular action will change the state in a particular
      way and a reward function that supplies the utility of each state and the
      cost of each action. A policy associates a decision with each possible
      state. The policy could be calculated (e.g., by iteration), be heuristic,
      or it can be learned. Game theory describes the rational behavior of
      multiple interacting agents and is used in AI programs that make decisions
      that involve other agents.
    </p>

    <h3>Learning</h3>

    <p>
      Machine learning is the study of programs that can improve their
      performance on a given task automatically. It has been a part of AI from
      the beginning. There are several kinds of machine learning. Unsupervised
      learning analyzes a stream of data and finds patterns and makes
      predictions without any other guidance. Supervised learning requires a
      human to label the input data first, and comes in two main varieties:
      classification (where the program must learn to predict what category the
      input belongs in) and regression (where the program must deduce a numeric
      function based on numeric input). In reinforcement learning, the agent is
      rewarded for good responses and punished for bad ones. The agent learns to
      choose responses that are classified as "good". Transfer learning is when
      the knowledge gained from one problem is applied to a new problem. Deep
      learning is a type of machine learning that runs inputs through
      biologically inspired artificial neural networks for all of these types of
      learning.
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >Computational learning theory</a
      >
      can assess learners by computational complexity, by sample complexity (how
      much data is required), or by other notions of optimization.
    </p>

    <h3>Natural language processing</h3>

    <p>
      Natural language processing (NLP) allows programs to read, write and
      communicate in human languages such as English. Specific problems include
      speech recognition, speech synthesis, machine translation, information
      extraction, information retrieval and question answering. Early work,
      based on Noam Chomsky's generative grammar and semantic networks, had
      difficulty with word-sense disambiguation unless restricted to small
      domains called "micro-worlds" (due to the common sense knowledge problem).
      Margaret Masterman believed that it was meaning and not grammar that was
      the key to understanding languages, and that thesauri and not dictionaries
      should be the basis of computational language structure. Modern deep
      learning techniques for NLP include word embedding (representing words,
      typically as vectors encoding their meaning), transformers (a deep
      learning architecture using an attention mechanism), and others. In 2019,
      generative pre-trained transformer (or "GPT") language models began to
      generate coherent text, and by 2023 these models were able to get
      human-level scores on the bar exam, SAT test, GRE test, and many other
      real-world applications.
    </p>

    <h3>perception</h3>
    <p>
      Machine perception is the ability to use input from sensors (such as
      cameras, microphones, wireless signals, active lidar, sonar, radar, and
      tactile sensors) to deduce aspects of the world. Computer vision is the
      ability to analyze visual input. The field includes speech recognition,
      image classification,facial recognition, object recognition, and robotic
      perception.
    </p>

    <h3>Social intelligence</h3>

    <p>
      Affective computing is an interdisciplinary umbrella that comprises
      systems that recognize, interpret, process, or simulate human feeling,
      emotion, and mood. For example, some virtual assistants are programmed to
      speak conversationally or even to banter humorously; it makes them appear
      more sensitive to the emotional dynamics of human interaction, or to
      otherwise facilitate human–computer interaction. However, this tends to
      give naïve users an unrealistic conception of the intelligence of existing
      computer agents. Moderate successes related to affective computing include
      textual sentiment analysis and, more recently, multimodal sentiment
      analysis, wherein AI classifies the affects displayed by a videotaped
      subject.
    </p>

    <h3>General intelligence</h3>
    <p>
      A machine with artificial general intelligence should be able to solve a
      wide variety of problems with breadth and versatility similar to human
      intelligence.
    </p>

    <h2>Techniques</h2>
    <hr />

    <h3>Search and optimization</h3>
    AI can solve many problems by intelligently searching through many possible
    solutions. There are two very different kinds of search used in AI: state
    space search and local search.

    <h3>State space search</h3>
    <p>
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >State space search</a
      >
      searches through a tree of possible states to try to find a goal state.
      For example, planning algorithms search through trees of goals and
      subgoals, attempting to find a path to a target goal, a process called
      means-ends analysis. Simple exhaustive searches are rarely sufficient for
      most real-world problems: the search space (the number of places to
      search) quickly grows to astronomical numbers. The result is a search that
      is too slow or never completes. "Heuristics" or "rules of thumb" can help
      prioritize choices that are more likely to reach a goal. Adversarial
      search is used for game-playing programs, such as chess or Go. It searches
      through a tree of possible moves and counter-moves, looking for a winning
      position.
    </p>

    <h3>Local search</h3>

    <p>
      Illustration of gradient descent for 3 different starting points. Two
      parameters (represented by the plan coordinates) are adjusted in order to
      minimize the loss function (the height). Local search uses mathematical
      optimization to find a solution to a problem. It begins with some form of
      guess and refines it incrementally. Gradient descent is a type of local
      search that optimizes a set of numerical parameters by incrementally
      adjusting them to minimize a loss function. Variants of gradient descent
      are commonly used to train neural networks. Another type of local search
      is evolutionary computation, which aims to iteratively improve a set of
      candidate solutions by "mutating" and "recombining" them, selecting only
      the fittest to survive each generation. Distributed search processes can
      coordinate via swarm intelligence algorithms. Two popular swarm algorithms
      used in search are particle swarm optimization (inspired by bird flocking)
      and ant colony optimization (inspired by ant trails).
    </p>

    <h3>Logic</h3>

    <p>
      Formal logic is used for reasoning and knowledge representation. Formal
      logic comes in two main forms: propositional logic (which operates on
      statements that are true or false and uses logical connectives such as
      "and", "or", "not" and "implies") and predicate logic (which also operates
      on objects, predicates and relations and uses quantifiers such as "Every X
      is a Y" and "There are some Xs that are Ys"). Deductive reasoning in logic
      is the process of proving a new statement (conclusion) from other
      statements that are given and assumed to be true (the premises). Proofs
      can be structured as proof trees, in which nodes are labelled by
      sentences, and children nodes are connected to parent nodes by inference
      rules. Given a problem and a set of premises, problem-solving reduces to
      searching for a proof tree whose root node is labelled by a solution of
      the problem and whose leaf nodes are labelled by premises or axioms. In
      the case of Horn clauses, problem-solving search can be performed by
      reasoning forwards from the premises or backwards from the problem. In the
      more general case of the clausal form of first-order logic, resolution is
      a single, axiom-free rule of inference, in which a problem is solved by
      proving a contradiction from premises that include the negation of the
      problem to be solved. Inference in both Horn clause logic and first-order
      logic is undecidable, and therefore intractable. However, backward
      reasoning with Horn clauses, which underpins computation in the logic
      programming language Prolog, is Turing complete. Moreover, its efficiency
      is competitive with computation in other symbolic programming languages.
      Fuzzy logic assigns a "degree of truth" between 0 and 1. It can therefore
      handle propositions that are vague and partially true. Non-monotonic
      logics, including logic programming with negation as failure, are designed
      to handle default reasoning. Other specialized versions of logic have been
      developed to describe many complex domains.
    </p>

    <h3>Probabilistic methods for uncertain reasoning</h3>

    <p>
      A simple Bayesian network, with the associated conditional probability
      tables Many problems in AI (including in reasoning, planning, learning,
      perception, and robotics) require the agent to operate with incomplete or
      uncertain information. AI researchers have devised a number of tools to
      solve these problems using methods from probability theory and economics.
      Precise mathematical tools have been developed that analyze how an agent
      can make choices and plan, using decision theory, decision analysis, and
      information value theory. These tools include models such as
      <strong>Markov decision processes</strong>, dynamic decision networks,
      game theory and mechanism design. Bayesian networks are a tool that can be
      used for reasoning (using the Bayesian inference algorithm), learning
      (using the expectation–maximization algorithm), planning (using decision
      networks) and perception (using dynamic Bayesian networks). Probabilistic
      algorithms can also be used for filtering, prediction, smoothing, and
      finding explanations for streams of data, thus helping perception systems
      analyze processes that occur over time (e.g., hidden Markov models or
      Kalman filters).
    </p>

    <h3>Classifiers and statistical learning methods</h3>

    <p>
      The simplest AI applications can be divided into two types: classifiers
      (e.g., "if shiny then diamond"), on one hand, and controllers (e.g., "if
      diamond then pick up"), on the other hand. Classifiers are functions that
      use
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >pattern matching</a
      >
      to determine the closest match. They can be fine-tuned based on chosen
      examples using supervised learning. Each pattern (also called an
      "observation") is labeled with a certain predefined class. All the
      observations combined with their class labels are known as a data set.
      When a new observation is received, that observation is classified based
      on previous experience. There are many kinds of classifiers in use. The
      decision tree is the simplest and most widely used symbolic machine
      learning algorithm. K-nearest neighbor algorithm was the most widely used
      analogical AI until the mid-1990s, and Kernel methods such as the support
      vector machine (SVM) displaced k-nearest neighbor in the 1990s. The naive
      Bayes classifier is reportedly the "most widely used learner" at Google,
      due in part to its scalability. Neural networks are also used as
      classifiers.
    </p>

    <h3>Artificial neural networks</h3>

    <p>
      A neural network is an interconnected group of nodes, akin to the vast
      network of neurons in the human brain. An artificial neural network is
      based on a collection of nodes also known as artificial neurons, which
      loosely model the neurons in a biological brain. It is trained to
      recognise patterns; once trained, it can recognise those patterns in fresh
      data. There is an input, at least one hidden layer of nodes and an output.
      Each node applies a function and once the weight crosses its specified
      threshold, the data is transmitted to the next layer. A network is
      typically called a deep neural network if it has at least 2 hidden layers.
      Learning algorithms for neural networks use local search to choose the
      weights that will get the right output for each input during training. The
      most common training technique is the backpropagation algorithm. Neural
      networks learn to model complex relationships between inputs and outputs
      and find patterns in data. In theory, a neural network can learn any
      function. In feedforward neural networks the signal passes in only one
      direction. Recurrent neural networks feed the output signal back into the
      input, which allows short-term memories of previous input events. Long
      short term memory is the most successful network architecture for
      recurrent networks. Perceptrons use only a single layer of neurons, deep
      learning uses multiple layers. Convolutional neural networks strengthen
      the connection between neurons that are "close" to each other—this is
      especially important in image processing, where a local set of neurons
      must identify an "edge" before the network can identify an object.
    </p>

    <h3>Deep learning</h3>

    <p>
      Deep learning uses several layers of neurons between the network's inputs
      and outputs. The multiple layers can progressively extract higher-level
      features from the raw input. For example, in image processing, lower
      layers may identify edges, while higher layers may identify the concepts
      relevant to a human such as digits, letters, or faces. Deep learning has
      profoundly improved the performance of programs in many important
      subfields of artificial intelligence, including<strong>
        computer vision, speech recognition, natural language processing, image
        classification</strong
      >, and others. The reason that deep learning performs so well in so many
      applications is not known as of 2023. The sudden success of deep learning
      in 2012–2015 did not occur because of some new discovery or theoretical
      breakthrough (deep neural networks and backpropagation had been described
      by many people, as far back as the 1950s) but because of two factors: the
      incredible increase in computer power (including the hundred-fold increase
      in speed by switching to GPUs) and the availability of vast amounts of
      training data, especially the giant curated datasets used for benchmark
      testing, such as ImageNet.
    </p>

    <h3>GPT</h3>
    <p>
      Generative pre-trained transformers (GPT) are large language models that
      are based on the semantic relationships between words in sentences
      (natural language processing). Text-based GPT models are pre-trained on a
      large corpus of text which can be from the internet. The pre-training
      consists in predicting the next token (a token being usually a word,
      subword, or punctuation). Throughout this pre-training, GPT models
      accumulate knowledge about the world, and can then generate human-like
      text by repeatedly predicting the next token. Typically, a subsequent
      training phase makes the model more truthful, useful and harmless, usually
      with a technique called reinforcement learning from human feedback (RLHF).
      Current GPT models are still prone to generating falsehoods called
      "hallucinations", although this can be reduced with RLHF and quality data.
      They are used in chatbots, which allow you to ask a question or request a
      task in simple text. Current models and services include: Gemini (formerly
      Bard), ChatGPT, Grok, Claude, Copilot and LLaMA. Multimodal GPT models can
      process different types of data (modalities) such as images, videos,
      sound, and text.
    </p>

    <h3>Specialized hardware and software</h3>

    <p>
      In the late 2010s, graphics processing units (GPUs) that were increasingly
      designed with AI-specific enhancements and used with specialized
      TensorFlow software had replaced previously used central processing unit
      (CPUs) as the dominant means for large-scale (commercial and academic)
      machine learning models' training. Specialized programming languages such
      as Prolog were used in early AI research, but general-purpose programming
      languages like Python have become predominant.
    </p>

    <h2>Applications</h2>
    <hr />

    <p>
      and machine learning technology is used in most of the essential
      applications of the 2020s, including: search engines (such as Google
      Search),
      <strong>targeting online advertisements, recommendation systems</strong>
      (offered by Netflix, YouTube or Amazon), driving internet traffic,
      targeted advertising (AdSense, Facebook), virtual assistants (such as Siri
      or Alexa), autonomous vehicles (including drones, ADAS and self-driving
      cars), automatic language translation (Microsoft Translator, Google
      Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace
      and Google's FaceNet) and image labeling .
    </p>

    <h3>Health and medicine</h3>

    <p>
      The application of AI in medicine and medical research has the potential
      to increase patient care and quality of life. Through the lens of the
      Hippocratic Oath, medical professionals are ethically compelled to use AI,
      if applications can more accurately diagnose and treat patients. For
      medical research, AI is an important tool for processing and integrating
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >big data</a
      >
      . This is particularly important for organoid and tissue engineering
      development which use microscopy imaging as a key technique in
      fabrication. It has been suggested that AI can overcome discrepancies in
      funding allocated to different fields of research. New AI tools can deepen
      the understanding of biomedically relevant pathways. For example,
      AlphaFold 2 (2021) demonstrated the ability to approximate, in hours
      rather than months, the 3D structure of a protein. In 2023, it was
      reported that AI-guided drug discovery helped find a class of antibiotics
      capable of killing two different types of drug-resistant bacteria. In
      2024, researchers used machine learning to accelerate the search for
      Parkinson's disease drug treatments. Their aim was to identify compounds
      that block the clumping, or aggregation, of alpha-synuclein (the protein
      that characterises Parkinson's disease). They were able to speed up the
      initial screening process ten-fold and reduce the cost by a thousand-fold.
    </p>

    <h3>Games</h3>

    <p>
      Game playing programs have been used since the 1950s to demonstrate and
      test AI's most advanced techniques. Deep Blue became the first computer
      chess-playing system to beat a reigning world chess champion, Garry
      Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition
      match, IBM's question answering system, Watson, defeated the two greatest
      Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant
      margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with
      Go champion Lee Sedol, becoming the first computer Go-playing system to
      beat a professional Go player without handicaps. Then in 2017 it defeated
      Ke Jie, who was the best Go player in the world. Other programs handle
      imperfect-information games, such as the poker-playing program Pluribus.
      DeepMind developed increasingly generalistic reinforcement learning
      models, such as with MuZero, which could be trained to play chess, Go, or
      Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in
      StarCraft II, a particularly challenging real-time strategy game that
      involves incomplete knowledge of what happens on the map. In 2021, an AI
      agent competed in a PlayStation Gran Turismo competition, winning against
      four of the world's best Gran Turismo drivers using deep reinforcement
      learning.
    </p>

    <h3>Finance</h3>
    <p>
      Finance is one of the fastest growing sectors where applied AI tools are
      being deployed: from retail online banking to investment advice and
      insurance, where automated "robot advisers" have been in use for some
      years. World Pensions experts like Nicolas Firzli insist it may be too
      early to see the emergence of highly innovative AI-informed financial
      products and services: "the deployment of AI tools will simply further
      automatise things: destroying tens of thousands of jobs in banking,
      financial planning, and pension advice in the process, but I’m not sure it
      will unleash a new wave of [e.g., sophisticated] pension innovation."
    </p>

    <h3>Military</h3>

    <p>
      Various countries are deploying AI military applications. The main
      applications enhance
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >command and control</a
      >
      , communications, sensors, integration and interoperability. Research is
      targeting intelligence collection and analysis, logistics, cyber
      operations, information operations, and semiautonomous and autonomous
      vehicles. AI technologies enable coordination of sensors and effectors,
      threat detection and identification, marking of enemy positions, target
      acquisition, coordination and deconfliction of distributed Joint Fires
      between networked combat vehicles involving manned and unmanned teams. AI
      was incorporated into military operations in Iraq and Syria. In November
      2023, US Vice President Kamala Harris disclosed a declaration signed by 31
      nations to set guardrails for the military use of AI. The commitments
      include using legal reviews to ensure the compliance of military AI with
      international laws, and being cautious and transparent in the development
      of this technology.
    </p>

    <h3>Generative AI</h3>

    <p>
      In the early 2020s, generative AI gained widespread prominence. In March
      2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it. The
      increasing realism and ease-of-use of AI-based text-to-image generators
      such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral
      AI-generated photos. Widespread attention was gained by a fake photo of
      Pope Francis wearing a white puffer coat, the fictional arrest of Donald
      Trump, and a hoax of an attack on the Pentagon, as well as the usage in
      professional creative arts.
    </p>

    <h3>Other industry-specific tasks</h3>
    <p>
      There are also thousands of successful AI applications used to solve
      specific problems for specific industries or institutions. In a 2017
      survey, one in five companies reported having incorporated "AI" in some
      offerings or processes. A few examples are energy storage, medical
      diagnosis, military logistics, applications that predict the result of
      judicial decisions, foreign policy, or supply chain management. In
      agriculture, AI has helped farmers identify areas that need irrigation,
      fertilization, pesticide treatments or increasing yield. Agronomists use
      AI to conduct research and development. AI has been used to predict the
      ripening time for crops such as tomatoes, monitor soil moisture, operate
      agricultural robots, conduct predictive analytics, classify livestock pig
      call emotions, automate greenhouses, detect diseases and pests, and save
      water. Artificial intelligence is used in astronomy to analyze increasing
      amounts of available data and applications, mainly for "classification,
      regression, clustering, forecasting, generation, discovery, and the
      development of new scientific insights" for example for discovering
      exoplanets, forecasting solar activity, and distinguishing between signals
      and instrumental effects in gravitational wave astronomy. It could also be
      used for activities in space such as space exploration, including analysis
      of data from space missions, real-time science decisions of spacecraft,
      space debris avoidance, and more autonomous operation.
    </p>

    <h2>Ethics</h2>
    <hr />

    <h3>Risks and harm</h3>
    <h4>Privacy and copyright</h4>

    <p>
      Machine-learning algorithms require large amounts of data. The techniques
      used to acquire this data have raised concerns about privacy, surveillance
      and copyright. Technology companies collect a wide range of data from
      their users, including online activity, geolocation data, video and audio.
      For example, in order to build
      <strong>speech recognition</strong> algorithms, Amazon has recorded
      millions of private conversations and allowed temporary workers to listen
      to and transcribe some of them. Opinions about this widespread
      surveillance range from those who see it as a necessary evil to those for
      whom it is clearly unethical and a violation of the right to privacy. AI
      developers argue that this is the only way to deliver valuable
      applications. and have developed several techniques that attempt to
      preserve privacy while still obtaining the data, such as data aggregation,
      de-identification and differential privacy. Since 2016, some privacy
      experts, such as Cynthia Dwork, have begun to view privacy in terms of
      fairness. Brian Christian wrote that experts have pivoted "from the
      question of 'what they know' to the question of 'what they're doing with
      it'." Generative AI is often trained on unlicensed copyrighted works,
      including in domains such as images or computer code; the output is then
      used under the rationale of "fair use". Experts disagree about how well
      and under what circumstances this rationale will hold up in courts of law;
      relevant factors may include "the purpose and character of the use of the
      copyrighted work" and "the effect upon the potential market for the
      copyrighted work". Website owners who do not wish to have their content
      scraped can indicate it in a "robots.txt" file. In 2023, leading authors
      (including John Grisham and Jonathan Franzen) sued AI companies for using
      their work to train generative AI. Another discussed approach is to
      envision a separate sui generis system of protection for creations
      generated by AI to ensure fair attribution and compensation for human
      authors.
    </p>

    <h4>Misinformation</h4>

    <p>
      YouTube, Facebook and others use recommender systems to guide users to
      more content. These AI programs were given the goal of maximizing user
      engagement (that is, the only goal was to keep people watching). The AI
      learned that users tended to choose misinformation, conspiracy theories,
      and extreme partisan content, and, to keep them watching, the AI
      recommended more of it. Users also tended to watch more content on the
      same subject, so the AI led people into filter bubbles where they received
      multiple versions of the same
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >misinformation</a
      >
      . This convinced many users that the misinformation was true, and
      ultimately undermined trust in institutions, the media and the government.
      The AI program had correctly learned to maximize its goal, but the result
      was harmful to society. After the U.S. election in 2016, major technology
      companies took steps to mitigate the problem. In 2022, generative AI began
      to create images, audio, video and text that are indistinguishable from
      real photographs, recordings, films or human writing. It is possible for
      bad actors to use this technology to create massive amounts of
      misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern
      about AI enabling "authoritarian leaders to manipulate their electorates"
      on a large scale, among other risks.
    </p>

    <h4>Algorithmic bias and fairness</h4>

    <p>
      Machine learning applications will be biased if they learn from biased
      data. The developers may not be aware that the bias exists. Bias can be
      introduced by the way training data is selected and by the way a model is
      deployed. If a biased algorithm is used to make decisions that can
      seriously harm people (as it can in medicine, finance, recruitment,
      housing or policing) then the algorithm may cause discrimination. Fairness
      in machine learning is the study of how to prevent the harm caused by
      algorithmic bias. It has become serious area of academic study within AI.
      Researchers have discovered it is not always possible to define "fairness"
      in a way that satisfies all stakeholders. On June 28, 2015, Google
      Photos's new image labeling feature mistakenly identified Jacky Alcine and
      a friend as "gorillas" because they were black. The system was trained on
      a dataset that contained very few images of black people, a problem called
      "sample size disparity". Google "fixed" this problem by preventing the
      system from labelling anything as a "gorilla". Eight years later, in 2023,
      Google Photos still could not identify a gorilla, and neither could
      similar products from Apple, Facebook, Microsoft and Amazon. COMPAS is a
      commercial program widely used by U.S. courts to assess the likelihood of
      a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica
      discovered that COMPAS exhibited racial bias, despite the fact that the
      program was not told the races of the defendants. Although the error rate
      for both whites and blacks was calibrated equal at exactly 61%, the errors
      for each race were different—the system consistently overestimated the
      chance that a black person would re-offend and would underestimate the
      chance that a white person would not re-offend. In 2017, several
      researchers[k] showed that it was mathematically impossible for COMPAS to
      accommodate all possible measures of fairness when the base rates of
      re-offense were different for whites and blacks in the data. A program can
      make biased decisions even if the data does not explicitly mention a
      problematic feature (such as "race" or "gender"). The feature will
      correlate with other features (like "address", "shopping history" or
      "first name"), and the program will make the same decisions based on these
      features as it would on "race" or "gender". Moritz Hardt said "the most
      robust fact in this research area is that fairness through blindness
      doesn't work." Criticism of COMPAS highlighted that machine learning
      models are designed to make "predictions" that are only valid if we assume
      that the future will resemble the past. If they are trained on data that
      includes the results of racist decisions in the past, machine learning
      models must predict that racist decisions will be made in the future. If
      an application then uses these predictions as recommendations, some of
      these "recommendations" will likely be racist. Thus, machine learning is
      not well suited to help make decisions in areas where there is hope that
      the future will be better than the past. It is necessarily descriptive and
      not proscriptive. Bias and unfairness may go undetected because the
      developers are overwhelmingly white and male: among AI engineers, about 4%
      are black and 20% are women. At its 2022 Conference on Fairness,
      Accountability, and Transparency (ACM FAccT 2022), the Association for
      Computing Machinery, in Seoul, South Korea, presented and published
      findings that recommend that until AI and robotics systems are
      demonstrated to be free of bias mistakes, they are unsafe, and the use of
      self-learning neural networks trained on vast, unregulated sources of
      flawed internet data should be curtailed.
    </p>

    <h4>Lack of transparency</h4>

    <p>
      Many AI systems are so complex that their designers cannot explain how
      they reach their decisions. Particularly with
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >deep neural networks</a
      >
      , in which there are a large amount of non-linear relationships between
      inputs and outputs. But some popular explainability techniques exist. It
      is impossible to be certain that a program is operating correctly if no
      one knows how exactly it works. There have been many cases where a machine
      learning program passed rigorous tests, but nevertheless learned something
      different than what the programmers intended. For example, a system that
      could identify skin diseases better than medical professionals was found
      to actually have a strong tendency to classify images with a ruler as
      "cancerous", because pictures of malignancies typically include a ruler to
      show the scale. Another machine learning system designed to help
      effectively allocate medical resources was found to classify patients with
      asthma as being at "low risk" of dying from pneumonia. Having asthma is
      actually a severe risk factor, but since the patients having asthma would
      usually get much more medical care, they were relatively unlikely to die
      according to the training data. The correlation between asthma and low
      risk of dying from pneumonia was real, but misleading. People who have
      been harmed by an algorithm's decision have a right to an explanation.
      Doctors, for example, are expected to clearly and completely explain to
      their colleagues the reasoning behind any decision they make. Early drafts
      of the European Union's General Data Protection Regulation in 2016
      included an explicit statement that this right exists.[m] Industry experts
      noted that this is an unsolved problem with no solution in sight.
      Regulators argued that nevertheless the harm is real: if the problem has
      no solution, the tools should not be used. DARPA established the XAI
      ("Explainable Artificial Intelligence") program in 2014 to try and solve
      these problems. There are several possible solutions to the transparency
      problem. SHAP tried to solve the transparency problems by visualising the
      contribution of each feature to the output. LIME can locally approximate a
      model with a simpler, interpretable model. Multitask learning provides a
      large number of outputs in addition to the target classification. These
      other outputs can help developers deduce what the network has learned.
      Deconvolution, DeepDream and other generative methods can allow developers
      to see what different layers of a deep network have learned and produce
      output that can suggest what the network is learning.
    </p>

    <h4>Bad actors and weaponized AI</h4>

    <p>
      Artificial intelligence provides a number of tools that are useful to bad
      actors, such as authoritarian governments, terrorists, criminals or rogue
      states. A lethal autonomous weapon is a machine that locates, selects and
      engages human targets without human supervision. Widely available AI tools
      can be used by bad actors to develop inexpensive autonomous weapons and,
      if produced at scale, they are potentially weapons of mass destruction.
      Even when used in conventional warfare, it is unlikely that they will be
      unable to reliably choose targets and could potentially kill an innocent
      person. In 2014, 30 nations (including China) supported a ban on
      autonomous weapons under the United Nations' Convention on Certain
      Conventional Weapons, however the United States and others disagreed. By
      2015, over fifty countries were reported to be researching battlefield
      robots. AI tools make it easier for authoritarian governments to
      efficiently control their citizens in several ways. Face and voice
      recognition allow widespread surveillance. Machine learning, operating
      this data, can classify potential enemies of the state and prevent them
      from hiding. Recommendation systems can precisely target propaganda and
      misinformation for maximum effect. Deepfakes and generative AI aid in
      producing misinformation. Advanced AI can make authoritarian centralized
      decision making more competitive than liberal and decentralized systems
      such as markets. It lowers the cost and difficulty of digital warfare and
      advanced spyware. All these technologies have been available since 2020 or
      earlier—AI facial recognition systems are already being used for mass
      surveillance in China. There many other ways that AI is expected to help
      bad actors, some of which can not be foreseen. For example,
      machine-learning AI is able to design tens of thousands of toxic molecules
      in a matter of hours.
    </p>

    <h4>Reliance on industry giants</h4>
    <p>
      Training AI systems requires an enormous amount of computing power.
      Usually only Big Tech companies have the financial resources to make such
      investments. Smaller startups such as Cohere and OpenAI end up buying
      access to data centers from Google and Microsoft respectively.
    </p>

    <h4>Technological unemployment</h4>

    <p>
      Economists have frequently highlighted the risks of redundancies from AI,
      and speculated about unemployment if there is no adequate social policy
      for full employment. In the past, technology has tended to increase rather
      than reduce total employment, but economists acknowledge that "we're in
      uncharted territory" with AI. A survey of economists showed disagreement
      about whether the increasing use of robots and AI will cause a substantial
      increase in long-term unemployment, but they generally agree that it could
      be a net benefit if productivity gains are redistributed. Risk estimates
      vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey
      estimated 47% of U.S. jobs are at "high risk" of potential automation,
      while an OECD report classified only 9% of U.S. jobs as "high risk". The
      methodology of speculating about future employment levels has been
      criticised as lacking evidential foundation, and for implying that
      technology, rather than social policy, creates unemployment, as opposed to
      redundancies. In April 2023, it was reported that 70% of the jobs for
      Chinese video game illustrators had been eliminated by generative
      artificial intelligence. Unlike previous waves of automation, many
      middle-class jobs may be eliminated by artificial intelligence; The
      Economist stated in 2015 that "the worry that AI could do to white-collar
      jobs what steam power did to blue-collar ones during the Industrial
      Revolution" is "worth taking seriously". Jobs at extreme risk range from
      paralegals to fast food cooks, while job demand is likely to increase for
      care-related professions ranging from personal healthcare to the clergy.
      From the early days of the development of artificial intelligence, there
      have been arguments, for example, those put forward by Joseph Weizenbaum,
      about whether tasks that can be done by computers actually should be done
      by them, given the difference between computers and humans, and between
      quantitative calculation and qualitative, value-based judgement.
    </p>

    <h4>Existential risk</h4>

    <p>
      It has been argued AI will become so powerful that humanity may
      irreversibly lose control of it. This could, as physicist Stephen Hawking
      stated, "spell the end of the human race". This scenario has been common
      in science fiction, when a computer or robot suddenly develops a
      human-like "self-awareness" (or "sentience" or "consciousness") and
      becomes a malevolent character. These sci-fi scenarios are misleading in
      several ways. First, AI does not require human-like "sentience" to be an
      existential risk. Modern AI programs are given specific goals and use
      learning and intelligence to achieve them. Philosopher Nick Bostrom argued
      that if one gives almost any goal to a sufficiently powerful AI, it may
      choose to destroy humanity to achieve it (he used the example of a
      paperclip factory manager). Stuart Russell gives the example of household
      robot that tries to find a way to kill its owner to prevent it from being
      unplugged, reasoning that "you can't fetch the coffee if you're dead." In
      order to be safe for humanity, a superintelligence would have to be
      genuinely aligned with humanity's morality and values so that it is
      "fundamentally on our side". Second, Yuval Noah Harari argues that AI does
      not require a robot body or physical control to pose an existential risk.
      The essential parts of civilization are not physical. Things like
      ideologies, law, government, money and the economy are made of language;
      they exist because there are stories that billions of people believe. The
      current prevalence of misinformation suggests that an AI could use
      language to convince people to believe anything, even to take actions that
      are destructive. The opinions amongst experts and industry insiders are
      mixed, with sizable fractions both concerned and unconcerned by risk from
      eventual superintelligent AI. Personalities such as Stephen Hawking, Bill
      Gates, and Elon Musk have expressed concern about existential risk from
      AI. AI pioneers including Fei-Fei Li, Geoffrey Hinton, Yoshua Bengio,
      Cynthia Breazeal, Rana el Kaliouby, Demis Hassabis, Joy Buolamwini, and
      Sam Altman have expressed concerns about the risks of AI. In 2023, many
      leading AI experts issued the joint statement that "Mitigating the risk of
      extinction from AI should be a global priority alongside other
      societal-scale risks such as pandemics and nuclear war". Other
      researchers, however, spoke in favor of a less dystopian view. AI pioneer
      Juergen Schmidhuber did not sign the joint statement, emphasising that in
      95% of all cases, AI research is about making "human lives longer and
      healthier and easier." While the tools that are now being used to improve
      lives can also be used by bad actors, "they can also be used against the
      bad actors." Andrew Ng also argued that "it's a mistake to fall for the
      doomsday hype on AI—and that regulators who do will only benefit vested
      interests." Yann LeCun "scoffs at his peers' dystopian scenarios of
      supercharged misinformation and even, eventually, human extinction." In
      the early 2010s, experts argued that the risks are too distant in the
      future to warrant research or that humans will be valuable from the
      perspective of a superintelligent machine. However, after 2016, the study
      of current and future risks and possible solutions became a serious area
      of research.
    </p>

    <h3>Ethical machines and alignment</h3>

    <p>
      Friendly AI are machines that have been designed from the beginning to
      minimize risks and to make choices that benefit humans. Eliezer Yudkowsky,
      who coined the term, argues that developing friendly AI should be a higher
      research priority: it may require a large investment and it must be
      completed before AI becomes an existential risk. Machines with
      intelligence have the potential to use their intelligence to make ethical
      decisions. The field of machine ethics provides machines with ethical
      principles and procedures for resolving ethical dilemmas. The field of
      machine ethics is also called computational morality, and was founded at
      an AAAI symposium in 2005. Other approaches include Wendell Wallach's
      "artificial moral agents" and Stuart J. Russell's three principles for
      developing provably beneficial machines.
    </p>

    <h3>Open source</h3>
    <p>
      Active organizations in the AI open-source community include Hugging Face,
      Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral
      or Stable Diffusion, have been made open-weight, meaning that their
      architecture and trained parameters (the "weights") are publicly
      available. Open-weight models can be freely fine-tuned, which allows
      companies to specialize them with their own data and for their own
      use-case. Open-weight models are useful for research and innovation but
      can also be misused. Since they can be fine-tuned, any built-in security
      measure, such as objecting to harmful requests, can be trained away until
      it becomes ineffective. Some researchers warn that future AI models may
      develop dangerous capabilities (such as the potential to drastically
      facilitate bioterrorism), and that once released on the Internet, they
      can't be deleted everywhere if needed. They recommend pre-release audits
      and cost-benefit analyses.
    </p>

    <h3>Frameworks</h3>

    <p>
      Artificial Intelligence projects can have their ethical permissibility
      tested while designing, developing, and implementing an AI system. An AI
      framework such as the Care and Act Framework containing the SUM
      values—developed by the <strong>Alan Turing Institute</strong> tests
      projects in four main areas RESPECT the dignity of individual people
      CONNECT with other people sincerely, openly and inclusively CARE for the
      wellbeing of everyone PROTECT social values, justice and the public
      interest Other developments in ethical frameworks include those decided
      upon during the Asilomar Conference, the Montreal Declaration for
      Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative,
      among others;[248] however, these principles do not go without their
      criticisms, especially regards to the people chosen contributes to these
      frameworks. Promotion of the wellbeing of the people and communities that
      these technologies affect requires consideration of the social and ethical
      implications at all stages of AI system design, development and
      implementation, and collaboration between job roles such as data
      scientists, product managers, data engineers, domain experts, and delivery
      managers. The AI Safety Institute in the UK has released a testing toolset
      called 'Inspect' for AI safety evaluations available under a MIT
      open-source licence which is freely available on GitHub and can be
      improved with third-party packages. It can be used to evaluate AI models
      in a range of areas including core knowledge, ability to reason, and
      autonomous capabilities.
    </p>

    <h2>History</h2>
    <hr />

    <p>
      The study of mechanical or "formal" reasoning began with philosophers and
      mathematicians in antiquity. The study of logic led directly to Alan
      Turing's <strong>theory of computation</strong>, which suggested that a
      machine, by shuffling symbols as simple as "0" and "1", could simulate any
      conceivable form of mathematical reasoning. This, along with concurrent
      discoveries in cybernetics, information theory and neurobiology, led
      researchers to consider the possibility of building an "electronic brain".
      They developed several areas of research that would become part of AI,
      such as McCullouch and Pitts design for "
      <a href="https://en.wikipedia.org/wiki/Artificial_intelligence"
        >artificial neurons</a
      >
      " in 1943, and Turing's influential 1950 paper 'Computing Machinery and
      Intelligence', which introduced the Turing test and showed that "machine
      intelligence" was plausible. The field of AI research was founded at a
      workshop at Dartmouth College in 1956.[r][6] The attendees became the
      leaders of AI research in the 1960s. They and their students produced
      programs that the press described as "astonishing":[t] computers were
      learning checkers strategies, solving word problems in algebra, proving
      logical theorems and speaking English. Artificial intelligence
      laboratories were set up at a number of British and U.S. Universities in
      the latter 1950s and early 1960s. Researchers in the 1960s and the 1970s
      were convinced that their methods would eventually succeed in creating a
      machine with general intelligence and considered this the goal of their
      field. Herbert Simon predicted, "machines will be capable, within twenty
      years, of doing any work a man can do". Marvin Minsky agreed, writing,
      "within a generation . the problem of creating 'artificial intelligence'
      will substantially be solved". They had, however, underestimated the
      difficulty of the problem.[v] In 1974, both the U.S. and British
      governments cut off exploratory research in response to the criticism of
      Sir James Lighthill and ongoing pressure from the U.S. Congress to<strong>
        fund more productive projects</strong
      >. Minsky's and Papert's book Perceptrons was understood as proving that
      artificial neural networks would never be useful for solving real-world
      tasks, thus discrediting the approach altogether. The "AI winter", a
      period when obtaining funding for AI projects was difficult, followed. In
      the early 1980s, AI research was revived by the commercial success of
      expert systems, a form of AI program that simulated the knowledge and
      analytical skills of human experts. By 1985, the market for AI had reached
      over a billion dollars. At the same time, Japan's fifth generation
      computer project inspired the U.S. and British governments to restore
      funding for academic research.[8] However, beginning with the collapse of
      the Lisp Machine market in 1987, AI once again fell into disrepute, and a
      second, longer-lasting winter began.[10] Up to this point, most of AI's
      funding had gone to projects that used high-level symbols to represent
      mental objects like plans, goals, beliefs, and known facts. In the 1980s,
      some researchers began to doubt that this approach would be able to
      imitate all the processes of human cognition, especially perception,
      robotics, learning and pattern recognition, and began to look into
      "sub-symbolic" approaches. Rodney Brooks rejected "representation" in
      general and focussed directly on engineering machines that move and
      survive.[w] Judea Pearl, Lofti Zadeh and others developed methods that
      handled incomplete and uncertain information by making reasonable guesses
      rather than precise logic. But the most important development was the
      revival of "connectionism", including neural network research, by Geoffrey
      Hinton and others. In 1990, Yann LeCun successfully showed that
      convolutional neural networks can recognize handwritten digits, the first
      of many successful applications of neural networks. AI gradually restored
      its reputation in the late 1990s and early 21st century by exploiting
      formal mathematical methods and by finding specific solutions to specific
      problems. This "narrow" and "formal" focus allowed researchers to produce
      verifiable results and collaborate with other fields (such as statistics,
      economics and mathematics).[293] By 2000, solutions developed by AI
      researchers were being widely used, although in the 1990s they were rarely
      described as "artificial intelligence".[294] However, several academic
      researchers became concerned that AI was no longer pursuing its original
      goal of creating versatile, fully intelligent machines. Beginning around
      2002, they founded the subfield of artificial general intelligence (or
      "AGI"), which had several well-funded institutions by the 2010s.
    </p>

    <h2>Philosophy</h2>
    <hr />

    <p>
      Defining artificial intelligence

      <strong> Alan Turing</strong> wrote in 1950 "I propose to consider the
      question 'can machines think'?" He advised changing the question from
      whether a machine "thinks", to "whether or not it is possible for
      machinery to show intelligent behaviour". He devised the Turing test,
      which measures the ability of a machine to simulate human conversation.
      Since we can only observe the behavior of the machine, it does not matter
      if it is "actually" thinking or literally has a "mind". Turing notes that
      we can not determine these things about other people but "it is usual to
      have a polite convention that everyone thinks" Russell and Norvig agree
      with Turing that intelligence must be defined in terms of external
      behavior, not internal structure.[1] However, they are critical that the
      test requires the machine to imitate humans. "<strong
        >Aeronautical engineering </strong
      >," they wrote, "do not define the goal of their field as making 'machines
      that fly so exactly like pigeons that they can fool other pigeons.'"[304]
      AI founder John McCarthy agreed, writing that "Artificial intelligence is
      not, by definition, simulation of human intelligence". McCarthy defines
      intelligence as "the computational part of the ability to achieve goals in
      the world".[306] Another AI founder, Marvin Minsky similarly describes it
      as "the ability to solve hard problems".[307] The leading AI textbook
      defines it as the study of agents that perceive their environment and take
      actions that maximize their chances of achieving defined goals.[1] These
      definitions view intelligence in terms of well-defined problems with
      well-defined solutions, where both the difficulty of the problem and the
      performance of the program are direct measures of the "intelligence" of
      the machine—and no other philosophical discussion is required, or may not
      even be possible. Another definition has been adopted by Google,[308] a
      major practitioner in the field of AI. This definition stipulates the
      ability of systems to synthesize information as the manifestation of
      intelligence, similar to the way it is defined in biological intelligence.
    </p>

    <h2>Future</h2>
    <hr />

    <p>
      A <strong>superintelligence</strong> is a hypothetical agent that would
      possess intelligence far surpassing that of the brightest and most gifted
      human mind. If research into artificial general intelligence produced
      sufficiently intelligent software, it might be able to reprogram and
      improve itself. The improved software would be even better at improving
      itself, leading to what I. J. Good called an "intelligence explosion" and
      Vernor Vinge called a "singularity". However, technologies cannot improve
      exponentially indefinitely, and typically follow an S-shaped curve,
      slowing when they reach the physical limits of what the technology can do.
      Transhumanism Robot designer Hans Moravec, cyberneticist Kevin Warwick,
      and inventor Ray Kurzweil have predicted that humans and machines will
      merge in the future into cyborgs that are more capable and powerful than
      either. This idea, called transhumanism, has roots in Aldous Huxley and
      Robert Ettinger.

      <strong>Edward Fredk</strong> argues that "artificial intelligence is the
      next stage in evolution", an idea first proposed by Samuel Butler's
      "Darwin among the Machines" as far back as 1863, and expanded upon by
      George Dyson in his book of the same name in 1998.
    </p>

    <iframe
      width="560"
      height="315"
      src="https://www.youtube.com/embed/oV74Najm6Nc?si=f1PwIL1MM2YZz7jM"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      referrerpolicy="strict-origin-when-cross-origin"
      allowfullscreen
    ></iframe>

    <h1>List</h1>
    <h1>Artificial Intelligence</h1>

    <h2>Key Components of AI</h2>

    <ol>
      <li>Machine Learning</li>

      <li>Deep Cleaning</li>

      <li>Neural Network</li>

      <li>Natural Language Processing</li>

      <li>Computer Vision</li>

      <li>Cognitive computing</li>
    </ol>

    <h2>Application of AI</h2>
    <ol>
      <li>Health and medicine</li>

      <li>Games</li>

      <li>Finance</li>

      <li>Military</li>

      <li>Generative AI</li>

      <li>Other industry-specific tasks</li>
    </ol>

    <h2>Types of AI</h2>
    <ol type="A" start="1">
      <li>Narrow AI</li>

      <li>Artificial General Intelligence</li>

      <li>Artificial Superintelligence</li>

      <li>Reactive Machine AI</li>

      <li>Limited Memory AI</li>

      <li>Theory of Mind AI</li>

      <li>Self-Aware AI</li>
    </ol>

    <h2>Artificial Intelligence (AI) Companies</h2>
    <ol>
      <li>OpenAI</li>

      <li>Google</li>

      <li>IBM</li>

      <li>Microsoft</li>

      <li>NVIDIA</li>

      <li>Amazon</li>

      <li>Anthropic</li>

      <li>Anduril</li>

      <li>Klaviyo</li>

      <li>Publica by IAS</li>
    </ol>
    <h2>ARTIFICIAL INTELLIGENCE EXAMPLES</h2>

    <ol type="I" start="1">
      <li>Manufacturing robots</li>

      <li>Self-driving cars</li>

      <li>Smart assistants</li>

      <li>Healthcare management</li>

      <li>Automated financial investing</li>

      <li>Virtual travel booking agent</li>

      <li>Social media monitoring</li>

      <li>Marketing chatbots</li>
    </ol>

    <h1>Table</h1>
    <h3>Types of Artificial Intelligence</h3>

    <table border="2">
      <tr>
        <th>Sr.no</th>
        <th>Types</th>
        <th>Define</th>
      </tr>

      <tr>
        <td>1</td>
        <td>Narrow AI</td>
        <td>
          Narrow AI, also known as artificial narrow intelligence (ANI) or weak
          AI, describes AI tools designed to carry out very specific actions or
          commands.
        </td>
      </tr>

      <tr>
        <td>2</td>
        <td>Artificial General Intelligence</td>
        <td>
          Artificial general intelligence (AGI), also called general AI or
          strong AI, describes AI that can learn, think and perform a wide range
          of actions similarly to humans.
        </td>
      </tr>

      <tr>
        <td>3</td>
        <td>Artificial Superintelligence</td>
        <td>
          Artificial superintelligence (ASI), or super AI, is the stuff of
          science fiction. It’s theorized that once AI has reached the general
          intelligence level, it will soon learn at such a fast rate that its
          knowledge and capabilities will become stronger than that even of
          humankind.
        </td>
      </tr>

      <tr>
        <td>4</td>
        <td>Reactive Machine AI</td>
        <td>
          Reactive machines are just that — reactionary. They can respond to
          immediate requests and tasks, but they aren’t capable of storing
          memory, learning from past experiences or improving their
          functionality through experiences.
        </td>
      </tr>
      <tr>
        <td>5</td>
        <td>Limited Memory AI</td>
        <td>
          Limited memory AI can store past data and use that data to make
          predictions. This means it actively builds its own limited, short-term
          knowledge base and performs tasks based on that knowledge.
        </td>
      </tr>

      <tr>
        <td>6</td>
        <td>Theory of Mind AI</td>
        <td>
          Theory of mind refers to the concept of AI that can perceive and pick
          up on the emotions of others. The term is borrowed from psychology,
          describing humans’ ability to read the emotions of others and predict
          future actions based on that information.
        </td>
      </tr>

      <tr>
        <td>7</td>
        <td>Self-Aware AI</td>
        <td>
          Self-aware AI describes artificial intelligence that possesses
          self-awareness. Referred to as the AI point of singularity, self-aware
          AI is the stage beyond theory of mind and is one of the ultimate goals
          in AI development.
        </td>
      </tr>
    </table>
  </body>
</html>
